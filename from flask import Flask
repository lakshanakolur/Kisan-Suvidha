from flask import Flask
app = Flask(__name__)

@app.route('/') 
def hello_world(): 
    return 'Hello World'
    #!/usr/bin/env python
	# coding: utf-8

	# In[1]:
	print("hey")
	import os
	print("hey")
	import sys
	import warnings
	warnings.filterwarnings("ignore")
	sys.path.append('/Users/keertankrishnan/anaconda3')
	import json

	print("hey")
	import pandas as pd
	print("hey1")

	# In[2]:


	import numpy as np


	# In[45]:


	def inp(location,time):
	    userloc = location
	    reqtime = time


	# In[3]:


	f = pd.read_csv('final.csv')
	#f


	# In[4]:


	from sklearn import preprocessing
	label_encoder = preprocessing.LabelEncoder()

	for i in range(f.shape[1]):
	    if i in [0,1,15,16]:
	        f.iloc[:,i]= label_encoder.fit_transform(f.iloc[:,i])
	        if i==1:
	            c = label_encoder.classes_
	#f


	# In[5]:


	min_max_scaler = preprocessing.MinMaxScaler()
	for i in range(3,15):
	    n = f.iloc[:,i].values #returns a numpy array
	    n = n.reshape(-1, 1)
	    f.iloc[:,i] = min_max_scaler.fit_transform(n)
	#f
	X = f
	    


	# In[6]:


	y = f.loc[:,['Crops']]
	y = np.asarray(y)
	from sklearn.preprocessing import OneHotEncoder
	onehot_encoder = OneHotEncoder(sparse=False)
	#y = y.reshape(len(y), 1)
	y = onehot_encoder.fit_transform(y)
	#len(y.transpose())


	# In[7]:


	def architecture(nneurons):
	    NN_ARCHITECTURE = []
	    NN_ARCHITECTURE.append({"input_dim": nneurons, "output_dim": nneurons*2+1, "activation": "sigmoid"})
	    NN_ARCHITECTURE.append({"input_dim": nneurons*2+1, "output_dim": 32, "activation": "sigmoid"})
	    return NN_ARCHITECTURE


	# In[8]:


	def init_layers(nn_architecture, seed = 80):
	    # random seed initiation
	    np.random.seed(seed)
	    # number of layers in our neural network
	    number_of_layers = len(nn_architecture)
	    # parameters storage initiation
	    params_values = {}
	    
	    # iteration over network layers
	    for idx, layer in enumerate(nn_architecture):
	        # we number network layers from 1
	        layer_idx = idx + 1
	        
	        # extracting the number of units in layers
	        layer_input_size = layer["input_dim"]
	        layer_output_size = layer["output_dim"]
	        
	        # initiating the values of the W matrix
	        # and vector b for subsequent layers
	        params_values['W' + str(layer_idx)] = np.random.randn(
	            layer_output_size, layer_input_size) * 0.1
	        params_values['b' + str(layer_idx)] = np.random.randn(
	            layer_output_size, 1) * 0.1
	        
	    return params_values


	# In[9]:


	def sigmoid(Z):
	    return 1/(1+np.exp(-Z))

	def relu(Z):
	    return np.maximum(0,Z)

	def sigmoid_backward(dA, Z):
	    sig = sigmoid(Z)
	    return dA * sig * (1 - sig)

	def relu_backward(dA, Z):
	    dZ = np.array(dA, copy = True)
	    dZ[Z <= 0] = 0;
	    return dZ;


	# In[10]:


	def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation="relu"):
	    # calculation of the input value for the activation function
	    Z_curr = np.dot(W_curr, A_prev) + b_curr
	    
	    # selection of activation function
	    if activation is "relu":
	        activation_func = relu
	    elif activation is "sigmoid":
	        activation_func = sigmoid
	    else:
	        raise Exception('Non-supported activation function')
	        
	    # return of calculated activation A and the intermediate Z matrix
	    return activation_func(Z_curr), Z_curr


	# In[11]:


	def full_forward_propagation(X, params_values, nn_architecture):
	    # creating a temporary memory to store the information needed for a backward step
	    memory = {}
	    # X vector is the activation for layer 0â€Š
	    A_curr = X
	    
	    # iteration over network layers
	    for idx, layer in enumerate(nn_architecture):
	        # we number network layers from 1
	        layer_idx = idx + 1
	        # transfer the activation from the previous iteration
	        A_prev = A_curr
	        
	        # extraction of the activation function for the current layer
	        activ_function_curr = layer["activation"]
	        # extraction of W for the current layer
	        W_curr = params_values["W" + str(layer_idx)]
	        # extraction of b for the current layer
	        b_curr = params_values["b" + str(layer_idx)]
	        # calculation of activation for the current layer
	        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)
	        
	        # saving calculated values in the memory
	        memory["A" + str(idx)] = A_prev
	        memory["Z" + str(layer_idx)] = Z_curr
	        A_curr[A_curr==1] = 0.99999999
	        A_curr[A_curr==0] = 0.00000001
	    # return of prediction vector and a dictionary containing intermediate values
	    return A_curr, memory


	# In[12]:


	def get_cost_value(Y_hat, Y):
	    # number of examples
	    m = Y_hat.shape[1]
	    # calculation of the cost according to the formula
	    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))
	    return np.squeeze(cost)


	# In[13]:


	# an auxiliary function that converts probability into class
	def convert_prob_into_class(probs):
	    probs_ = np.copy(probs)
	    probs_[probs_ > 0.5] = 1
	    probs_[probs_ <= 0.5] = 0
	    return probs_


	# In[14]:


	def get_accuracy_value(Y_hat, Y):
	    Y_hat_ = convert_prob_into_class(Y_hat)
	    return (Y_hat_ == Y).all(axis=0).mean()


	# In[15]:


	def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation="relu"):
	    # number of examples
	    m = A_prev.shape[1]
	    #print("Number of shitz" + str(m))
	    
	    # selection of activation function
	    if activation is "relu":
	        backward_activation_func = relu_backward
	    elif activation is "sigmoid":
	        backward_activation_func = sigmoid_backward
	    else:
	        raise Exception('Non-supported activation function')
	    
	    # calculation of the activation function derivative
	    dZ_curr = backward_activation_func(dA_curr, Z_curr)
	    
	    # derivative of the matrix W
	    dW_curr = np.dot(dZ_curr, A_prev.T) / m
	    # derivative of the vector b
	    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m
	    # derivative of the matrix A_prev
	    dA_prev = np.dot(W_curr.T, dZ_curr)

	    return dA_prev, dW_curr, db_curr


	# In[16]:


	def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):
	    grads_values = {}
	    
	    # number of examples
	    m = Y.shape[1]
	    # a hack ensuring the same shape of the prediction vector and labels vector
	    Y = Y.reshape(Y_hat.shape)
	    
	    # initiation of gradient descent algorithm
	    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));
	    
	    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):
	        # we number network layers from 1
	        layer_idx_curr = layer_idx_prev + 1
	        # extraction of the activation function for the current layer
	        activ_function_curr = layer["activation"]
	        
	        dA_curr = dA_prev
	        
	        A_prev = memory["A" + str(layer_idx_prev)]
	        Z_curr = memory["Z" + str(layer_idx_curr)]
	        
	        W_curr = params_values["W" + str(layer_idx_curr)]
	        b_curr = params_values["b" + str(layer_idx_curr)]
	        
	        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(
	            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)
	        
	        grads_values["dW" + str(layer_idx_curr)] = dW_curr
	        grads_values["db" + str(layer_idx_curr)] = db_curr
	    
	    return grads_values


	# In[17]:


	def update(params_values, grads_values, nn_architecture, learning_rate):

	    # iteration over network layers
	    for layer_idx, layer in enumerate(nn_architecture, 1):
	        params_values["W" + str(layer_idx)] -= learning_rate * grads_values["dW" + str(layer_idx)]        
	        params_values["b" + str(layer_idx)] -= learning_rate * grads_values["db" + str(layer_idx)]

	    return params_values;


	# In[18]:


	def argmax(h):
	    return np.random.randint(0,32)


	# In[19]:


	def train_BP(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):
	    # initiation of neural net parameters
	    params_values = init_layers(nn_architecture)
	    # initiation of lists storing the history 
	    # of metrics calculated during the learning process 
	    cost_history = []
	    accuracy_history = []
	    
	    # performing calculations for subsequent iterations
	    for i in range(epochs):
	        # step forward
	        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)
	        f = open("weights.txt",'w')
	        f.write(str(cashe))
	        f.close()
	        # calculating metrics and saving them in history
	        cost = get_cost_value(Y_hat, Y)
	        cost_history.append(cost)
	        accuracy = get_accuracy_value(Y_hat, Y)
	        accuracy_history.append(accuracy)
	        
	        # step backward - calculating gradient
	        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)
	        # updating model state
	        params_values = update(params_values, grads_values, nn_architecture, learning_rate)
	        
	        """if(i % 50 == 0):
	            if(verbose):
	                print("Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}".format(i, cost, accuracy))
	            if(callback is not None):
	                callback(i, params_values)"""
	            
	    return params_values


	# In[20]:


	import os
	from sklearn.datasets import make_moons
	from sklearn.model_selection import train_test_split

	import seaborn as sns
	import matplotlib.pyplot as plt
	from matplotlib import cm
	from mpl_toolkits.mplot3d import Axes3D
	sns.set_style("whitegrid")

	stderr = sys.stderr
	sys.stderr = open(os.devnull, 'w')
	import keras
	sys.stderr = stderr
	from keras.models import Sequential
	from keras.layers import Dense
	from keras.utils import np_utils
	from keras import regularizers

	from sklearn.metrics import accuracy_score


	# In[40]:


	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0025, random_state=24)


	# In[36]:


	NN_ARCHITECTURE = architecture(X_train.shape[1])
	params_values = train_BP(np.transpose(X_train), np.transpose(y_train), NN_ARCHITECTURE, 30, 0.01,True)


	# In[41]:


	Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), params_values, NN_ARCHITECTURE)


	# In[42]:


	X_test = pd.DataFrame(X_test)
	X_test.iloc[:,2:]


	# In[43]:



	y_test_new = Y_test_hat.transpose()
	l = []
	#print(y_test_new.shape)
	#print(y_test_new)
	for k,i in enumerate(y_test_new):
	    l .append(list(c[np.argsort(i)[-3:]]))
	print(str(l))
	f = open('new.txt','w')
	f.write(str(l))
	f.close()
	#print(str(l))
	#y = json.loads(str(l))
	#print(y)


	# In[ ]:






  
if __name__ == '__main__': 
    app.run() 